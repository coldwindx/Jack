{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhulin/anaconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "import lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))  \n",
    "pl.seed_everything(42, workers=True)\n",
    "torch.set_float32_matmul_precision(precision=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"pretrain\": '/home/zhulin/pretrain/bert_pretrain_uncased/',\n",
    "    \"model\": \"./SingleChannelPredictor.pt\",\n",
    "    \"dataset\": \"/home/zhulin/datasets/cdatasets.test.5.csv\",\n",
    "    \"batch_size\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        # 设置用于测量时间的 cuda Event, 这是PyTorch 官方推荐的接口,理论上应该最靠谱\n",
    "        self.starter = torch.cuda.Event(enable_timing=True)\n",
    "        self.ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    def warmup(self, interface, *args, **kwargs):\n",
    "        # 预热, GPU 平时可能为了节能而处于休眠状态, 因此需要预热\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                interface(*args, **kwargs)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    def measure(self, interface, *args, **kwargs):\n",
    "        timeings = []\n",
    "        for _ in tqdm(range(self.epochs)):\n",
    "            self.starter.record()\n",
    "            interface(*args, **kwargs)\n",
    "            self.ender.record()\n",
    "            torch.cuda.synchronize() # 等待GPU任务完成\n",
    "            t = self.starter.elapsed_time(self.ender)\n",
    "            \n",
    "            timeings.append(t)\n",
    "        return timeings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(args[\"pretrain\"], use_fast=True)\n",
    "predictor = torch.jit.load(args[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### load datasets\n",
    "import numpy as np\n",
    "import datatable as dt\n",
    "\n",
    "data = dt.fread(args[\"dataset\"], fill=True, max_nrows=128 * args[\"batch_size\"]).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def interface(tokenizer, predictor, data, batchsize):\n",
    "    for l in range(0, len(data), batchsize):\n",
    "        padded_sent_seq = tokenizer(data.iloc[l:l+batchsize][\"channel\"].to_list(), padding=True, truncation=True, max_length=2048, return_tensors=\"pt\")\n",
    "        predictor(padded_sent_seq[\"input_ids\"].cuda(), padded_sent_seq[\"attention_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:38<00:00,  2.41s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/core/predictor.py\", line 25, in forward\n    add_positional_encoding: bool=True) -> Tensor:\n    net = self.net\n    _0 = (net).forward(x, mask, add_positional_encoding, )\n          ~~~~~~~~~~~~ <--- HERE\n    return _0\n  File \"code/__torch__/core/transformer.py\", line 24, in forward\n      x1 = x0\n    transformer = self.transformer\n    x3 = (transformer).forward(x1, mask, )\n          ~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    pooling_net = self.pooling_net\n    x4 = (pooling_net).forward(x3, mask, )\n  File \"code/__torch__/core/transformer.py\", line 51, in forward\n    layers = self.layers\n    _0 = getattr(layers, \"0\")\n    return (_0).forward(x, mask, )\n            ~~~~~~~~~~~ <--- HERE\nclass EncoderBlock(Module):\n  __parameters__ = []\n  File \"code/__torch__/core/transformer.py\", line 66, in forward\n    mask: Optional[Tensor]=None) -> Tensor:\n    attn = self.attn\n    attn_out, _1, = (attn).forward(x, mask, )\n                     ~~~~~~~~~~~~~ <--- HERE\n    dropout = self.dropout\n    x5 = torch.add(x, (dropout).forward(attn_out, ))\n  File \"code/__torch__/core/transformer.py\", line 100, in forward\n    qkv1 = torch.permute(qkv0, [0, 2, 1, 3])\n    q, k, v, = torch.chunk(qkv1, 3, -1)\n    values, attention, = _2(q, k, v, mask, )\n                         ~~ <--- HERE\n    values0 = torch.permute(values, [0, 2, 1, 3])\n    values1 = torch.reshape(values0, [batch_size, seq_length, embed_dim])\n  File \"code/__torch__/core/transformer.py\", line 112, in scaled_dot_product\n  d_k = (torch.size(q))[-1]\n  attn_logits = torch.matmul(q, torch.transpose(k, -2, -1))\n  attn_logits0 = torch.div(attn_logits, torch.sqrt(d_k))\n                 ~~~~~~~~~ <--- HERE\n  if torch.__isnot__(mask, None):\n    mask0 = unchecked_cast(Tensor, mask)\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/home/zhulin/workspace/Jack/core/predictor.py\", line 23, in forward\n    def forward(self, x, mask: Optional [torch.Tensor]=None, add_positional_encoding: bool=True):\n         return self.net(x, mask=mask, add_positional_encoding=add_positional_encoding)\n                ~~~~~~~~ <--- HERE\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 169, in forward\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)              # [Batch, SeqLen, ModDim]\n            ~~~~~~~~~~~~~~~~ <--- HERE\n        x = self.pooling_net(x, mask=mask)              # GlobalAveragePooling\n        x = self.output_net(x)\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 104, in forward\n    def forward(self, x, mask: Optional [torch.Tensor]=None):\n        for layer in self.layers:\n            x = layer(x, mask=mask)\n                ~~~~~ <--- HERE\n        return x\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 87, in forward\n    def forward(self, x, mask: Optional [torch.Tensor]=None):\n        # Attention part\n        attn_out, _ = self.attn(x, mask=mask)\n                      ~~~~~~~~~ <--- HERE\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 51, in forward\n        q, k, v = qkv.chunk(3, dim=-1)\n    \n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n                            ~~~~~~~~~~~~~~~~~~ <--- HERE\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, embed_dim)\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 23, in scaled_dot_product\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    if mask is not None:\n        attn_mask = mask.unsqueeze(1).unsqueeze(2)\nRuntimeError: CUDA out of memory. Tried to allocate 1.61 GiB. GPU 0 has a total capacty of 11.66 GiB of which 907.19 MiB is free. Process 19592 has 2.98 GiB memory in use. Process 23209 has 4.78 GiB memory in use. Including non-PyTorch memory, this process has 2.95 GiB memory in use. Of the allocated memory 1.65 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mfread(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m], fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m \u001b[38;5;241m*\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m---> 10\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     times\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(t)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(t))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(times)\n",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m, in \u001b[0;36mTimer.measure\u001b[0;34m(self, interface, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarter\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m---> 19\u001b[0m     \u001b[43minterface\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mender\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m     21\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize() \u001b[38;5;66;03m# 等待GPU任务完成\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36minterface\u001b[0;34m(tokenizer, predictor, data, batchsize)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data), batchsize):\n\u001b[1;32m      4\u001b[0m     padded_sent_seq \u001b[38;5;241m=\u001b[39m tokenizer(data\u001b[38;5;241m.\u001b[39miloc[l:l\u001b[38;5;241m+\u001b[39mbatchsize][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list(), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sent_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_sent_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/core/predictor.py\", line 25, in forward\n    add_positional_encoding: bool=True) -> Tensor:\n    net = self.net\n    _0 = (net).forward(x, mask, add_positional_encoding, )\n          ~~~~~~~~~~~~ <--- HERE\n    return _0\n  File \"code/__torch__/core/transformer.py\", line 24, in forward\n      x1 = x0\n    transformer = self.transformer\n    x3 = (transformer).forward(x1, mask, )\n          ~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    pooling_net = self.pooling_net\n    x4 = (pooling_net).forward(x3, mask, )\n  File \"code/__torch__/core/transformer.py\", line 51, in forward\n    layers = self.layers\n    _0 = getattr(layers, \"0\")\n    return (_0).forward(x, mask, )\n            ~~~~~~~~~~~ <--- HERE\nclass EncoderBlock(Module):\n  __parameters__ = []\n  File \"code/__torch__/core/transformer.py\", line 66, in forward\n    mask: Optional[Tensor]=None) -> Tensor:\n    attn = self.attn\n    attn_out, _1, = (attn).forward(x, mask, )\n                     ~~~~~~~~~~~~~ <--- HERE\n    dropout = self.dropout\n    x5 = torch.add(x, (dropout).forward(attn_out, ))\n  File \"code/__torch__/core/transformer.py\", line 100, in forward\n    qkv1 = torch.permute(qkv0, [0, 2, 1, 3])\n    q, k, v, = torch.chunk(qkv1, 3, -1)\n    values, attention, = _2(q, k, v, mask, )\n                         ~~ <--- HERE\n    values0 = torch.permute(values, [0, 2, 1, 3])\n    values1 = torch.reshape(values0, [batch_size, seq_length, embed_dim])\n  File \"code/__torch__/core/transformer.py\", line 112, in scaled_dot_product\n  d_k = (torch.size(q))[-1]\n  attn_logits = torch.matmul(q, torch.transpose(k, -2, -1))\n  attn_logits0 = torch.div(attn_logits, torch.sqrt(d_k))\n                 ~~~~~~~~~ <--- HERE\n  if torch.__isnot__(mask, None):\n    mask0 = unchecked_cast(Tensor, mask)\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/home/zhulin/workspace/Jack/core/predictor.py\", line 23, in forward\n    def forward(self, x, mask: Optional [torch.Tensor]=None, add_positional_encoding: bool=True):\n         return self.net(x, mask=mask, add_positional_encoding=add_positional_encoding)\n                ~~~~~~~~ <--- HERE\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 169, in forward\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)              # [Batch, SeqLen, ModDim]\n            ~~~~~~~~~~~~~~~~ <--- HERE\n        x = self.pooling_net(x, mask=mask)              # GlobalAveragePooling\n        x = self.output_net(x)\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 104, in forward\n    def forward(self, x, mask: Optional [torch.Tensor]=None):\n        for layer in self.layers:\n            x = layer(x, mask=mask)\n                ~~~~~ <--- HERE\n        return x\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 87, in forward\n    def forward(self, x, mask: Optional [torch.Tensor]=None):\n        # Attention part\n        attn_out, _ = self.attn(x, mask=mask)\n                      ~~~~~~~~~ <--- HERE\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 51, in forward\n        q, k, v = qkv.chunk(3, dim=-1)\n    \n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n                            ~~~~~~~~~~~~~~~~~~ <--- HERE\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, embed_dim)\n  File \"/home/zhulin/workspace/Jack/core/transformer.py\", line 23, in scaled_dot_product\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    if mask is not None:\n        attn_mask = mask.unsqueeze(1).unsqueeze(2)\nRuntimeError: CUDA out of memory. Tried to allocate 1.61 GiB. GPU 0 has a total capacty of 11.66 GiB of which 907.19 MiB is free. Process 19592 has 2.98 GiB memory in use. Process 23209 has 4.78 GiB memory in use. Including non-PyTorch memory, this process has 2.95 GiB memory in use. Of the allocated memory 1.65 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "predictor.cuda().eval()\n",
    "timer = Timer(epochs=16)\n",
    "times = []\n",
    "\n",
    "data = dt.fread(args[\"dataset\"], fill=True, max_nrows=128 * 8).to_pandas()\n",
    "timer.warmup(interface, tokenizer, predictor, data, 8)\n",
    "\n",
    "for batch_size in range(8, 64, 8):\n",
    "    data = dt.fread(args[\"dataset\"], fill=True, max_nrows=128 * batch_size).to_pandas()\n",
    "    t = timer.measure(interface, tokenizer, predictor, data, batch_size)\n",
    "    times.append(sum(t)/len(t))\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-12 16:24:55.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m[+] warm up ...\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-12 16:25:16.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mtesting ...\n",
      "\u001b[0m\n",
      "100%|██████████| 100/100 [03:16<00:00,  1.97s/it]\n",
      "\u001b[32m2024-09-12 16:28:32.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m\n",
      "avg=1964.0056689453124\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "predictor.cuda().eval()\n",
    "# 预热, GPU 平时可能为了节能而处于休眠状态, 因此需要预热\n",
    "logger.info('[+] warm up ...\\n')\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        # _ = predictor(dummy_input)\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 设置用于测量时间的 cuda Event, 这是PyTorch 官方推荐的接口,理论上应该最靠谱\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "# 初始化一个时间容器\n",
    "timings = np.zeros((100, 1))\n",
    "\n",
    "logger.info('testing ...\\n')\n",
    "with torch.no_grad():\n",
    "    for rep in tqdm(range(100)):\n",
    "        starter.record()\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize() # 等待GPU任务完成\n",
    "        curr_time = starter.elapsed_time(ender) # 从 starter 到 ender 之间用时,单位为毫秒\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "avg = timings.sum()/100/1000\n",
    "logger.info('\\navg={}s\\n'.format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-12 16:47:30.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m[+] warm up ...\n",
      "\u001b[0m\n",
      "STAGE:2024-09-12 16:47:50 38310:38310 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-09-12 16:47:53 38310:38310 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-09-12 16:47:53 38310:38310 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        81.66%        1.699s       100.00%        2.081s        2.081s             1  \n",
      "                                                forward         1.24%      25.872ms        16.56%     344.648ms       2.693ms           128  \n",
      "                                           aten::linear         0.64%      13.221ms         5.02%     104.495ms     116.624us           896  \n",
      "                                            aten::addmm         2.98%      62.067ms         3.65%      75.898ms      84.708us           896  \n",
      "                                           aten::matmul         0.38%       7.808ms         2.57%      53.505ms     209.004us           256  \n",
      "                                          aten::reshape         0.47%       9.690ms         1.99%      41.399ms      26.952us          1536  \n",
      "                                            aten::copy_         0.76%      15.745ms         1.89%      39.307ms      43.869us           896  \n",
      "                                       cudaLaunchKernel         1.86%      38.772ms         1.86%      38.772ms      10.818us          3584  \n",
      "                                            aten::clone         0.23%       4.833ms         1.71%      35.514ms      55.491us           640  \n",
      "                                               aten::to         0.26%       5.487ms         1.57%      32.760ms      51.188us           640  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.081s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "logger.info('[+] warm up ...\\n')\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        # _ = predictor(dummy_input)\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
