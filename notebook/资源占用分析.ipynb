{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhulin/anaconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "import lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))  \n",
    "pl.seed_everything(42, workers=True)\n",
    "torch.set_float32_matmul_precision(precision=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"pretrain\": '/home/zhulin/pretrain/bert_pretrain_uncased/',\n",
    "    \"model\": \"./SingleChannelPredictor.pt\",\n",
    "    \"dataset\": \"/home/zhulin/datasets/cdatasets.test.5.csv\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model\n",
    "from core.predictor import SingleChannelPredictor\n",
    "tokenizer = BertTokenizerFast.from_pretrained(args[\"pretrain\"], use_fast=True)\n",
    "predictor = torch.jit.load(args[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### load datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "\n",
    "data = dt.fread(args[\"dataset\"], fill=True, max_nrows=1024).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def interface(tokenizer, predictor, data, batchsize):\n",
    "    n = len(data)\n",
    "    for l in range(0, n, batchsize):\n",
    "        padded_sent_seq = tokenizer(data.iloc[l:l+batchsize][\"channel\"].to_list(), padding=True, truncation=True, max_length=2048, return_tensors=\"pt\")\n",
    "        pred = predictor(padded_sent_seq[\"input_ids\"].cuda(), padded_sent_seq[\"attention_mask\"].cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-12 16:24:55.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m[+] warm up ...\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-12 16:25:16.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mtesting ...\n",
      "\u001b[0m\n",
      "100%|██████████| 100/100 [03:16<00:00,  1.97s/it]\n",
      "\u001b[32m2024-09-12 16:28:32.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m\n",
      "avg=1964.0056689453124\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "predictor.cuda().eval()\n",
    "# 预热, GPU 平时可能为了节能而处于休眠状态, 因此需要预热\n",
    "logger.info('[+] warm up ...\\n')\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        # _ = predictor(dummy_input)\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 设置用于测量时间的 cuda Event, 这是PyTorch 官方推荐的接口,理论上应该最靠谱\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "# 初始化一个时间容器\n",
    "timings = np.zeros((100, 1))\n",
    "\n",
    "logger.info('testing ...\\n')\n",
    "with torch.no_grad():\n",
    "    for rep in tqdm(range(100)):\n",
    "        starter.record()\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize() # 等待GPU任务完成\n",
    "        curr_time = starter.elapsed_time(ender) # 从 starter 到 ender 之间用时,单位为毫秒\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "avg = timings.sum()/100\n",
    "logger.info('\\navg={}\\n'.format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-12 16:31:29.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m[+] warm up ...\n",
      "\u001b[0m\n",
      "STAGE:2024-09-12 16:31:49 38310:38310 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-09-12 16:31:53 38310:38310 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-09-12 16:31:53 38310:38310 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        41.39%        1.388s       100.00%        3.353s        3.353s             1  \n",
      "                                                forward         9.68%     324.656ms        54.26%        1.819s       1.776ms          1024  \n",
      "                                           aten::linear         2.23%      74.924ms        14.30%     479.448ms      66.887us          7168  \n",
      "                                            aten::addmm         7.36%     246.771ms         9.70%     325.220ms      45.371us          7168  \n",
      "                                           aten::matmul         1.34%      44.933ms         6.12%     205.284ms     100.236us          2048  \n",
      "                                       cudaLaunchKernel         5.19%     174.015ms         5.19%     174.015ms       7.080us         24577  \n",
      "                                      fallback_function         0.80%      26.706ms         4.31%     144.532ms      47.048us          3072  \n",
      "                                               aten::to         0.82%      27.440ms         3.77%     126.327ms      24.673us          5120  \n",
      "                                       aten::layer_norm         0.34%      11.509ms         3.66%     122.770ms      59.946us          2048  \n",
      "                                            aten::copy_         1.27%      42.454ms         3.46%     116.002ms      28.321us          4096  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.353s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "logger.info('[+] warm up ...\\n')\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        # _ = predictor(dummy_input)\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        interface(tokenizer, predictor, data, 8)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
